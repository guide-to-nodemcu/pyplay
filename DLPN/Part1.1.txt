Artificial intelligence also includes fields where machine is perceived to be intelligent, even a pattern matching algorithm would do. Expert systems fall in this category.

Machine learning  on the other does something thats not explicitly, coded. a subset of AI.

It is easier to check if the image is saturated or not using the HSV values.


What is deep learning ?

The specification of what a layer does to its input data is stored in the layer’s
weights, which in essence are a bunch of numbers. In technical terms, we’d say that the
transformation implemented by a layer is parameterized by its weights.
(Weights are also sometimes called the parameters of a layer.) 

In this context, learning means finding a set of values for the weights of all layers in a network, such that the
network will correctly map example inputs to their associated targets.

input X > Layer of transformation > output Y
                                ^
                           weights

To control the output of a neural network, you need to be able to measure how far this output is from what you
expected. This is the job of the loss function of the network, also called the objective
function. 

The loss function takes the predictions of the network and the true target
(what you wanted the network to output) and computes a distance score.


input X > Layer of transformation > output Y > Loss function >  Loss score
                                ^                                                ^
                           weights                                      True output Y


This loss score is used by the optimizer to adjust weights.



input X > Layer of transformation > output Y > Loss function >  Loss score  > optimiser > |
                                ^                                                ^                                                            |
                           weights                                      True output Y                                              |
                                ^---------------------------------------------------------------------------<

Initially, the weights of the network are assigned random values, so the network
merely implements a series of random transformations. Naturally, its output is far
from what it should ideally be, and the loss score is accordingly very high. 

But with every example the network processes, the weights are adjusted a little in the correct
direction, and the loss score decreases. This is the training loop, which, repeated a sufficient number of times (typically tens of iterations over thousands of examples), yields
weight values that minimize the loss function. 

A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network

The risk with high expectations for the short term is that, as technology fails to deliver,
research investment will dry up, slowing progress for a long time.

Deep learning isn’t always the right tool for the
job—sometimes there isn’t enough data for deep learning to be applicable, and sometimes the problem is better solved by a different algorithm. 

If deep learning is your first contact with machine learning, then you may find yourself in a situation where all you have is the deep-learning hammer, and every machine-learning problem starts to
look like a nail.

What is probabilistic modelling ?

Probabilistic modeling is the application of the principles of statistics to data analysis.

One of the best-known algorithms in this category is the Naive Bayes algorithm.
Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theorem while assuming that the features in the input data are all independent (a strong, or “naive” assumption, which is where the name comes from).

A closely related model is the logistic regression (logreg for short), which is sometimes considered to be the “hello world” of modern machine learning. Don’t be misled by its name—logreg is a classification algorithm rather than a regression
algorithm.
It’s often the first thing a data scientist will try on a dataset to get a feel for the classification task at hand.

The first successful practical application of neural nets came in 1989 from Bell
Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks
and backpropagation, and applied them to the problem of classifying handwritten
digits. The resulting network, dubbed LeNet, was used by the United States Postal Service in the 1990s to automate the reading of ZIP codes on mail envelopes. 

What are Kernel Methods ?

Kernel methods are a group of
classification algorithms, the best known of which is the support vector machine (SVM).

What is an SVM ?

SVMs aim at solving classification problems by finding good
decision boundaries  between two sets of points
belonging to two different categories. A decision boundary can
be thought of as a line or surface separating your training data
into two spaces corresponding to two categories. To classify new
data points, you just need to check which side of the decision
boundary they fall on.

SVMs proceed to find these boundaries in two steps:

	1. 
The data is mapped to a new high-dimensional representation where the


decision boundary can be expressed as a hyperplane (if the data was twodimensional, a hyperplane would be a straight line).

	1. 
A good decision boundary (a separation hyperplane) is computed by trying to


maximize the distance between the hyperplane and the closest data points from
each class, a step called maximizing the margin. This allows the boundary to generalize well to new samples outside of the training dataset.

The technique of mapping data to a high-dimensional representation where a classification problem becomes simpler may look good on paper, but in practice it’s often computationally intractable.That’s where the kernel trick comes in (the key idea
that kernel methods are named after).

What is a Kernal function ?

Here’s the gist of it: to find good decision  hyperplanes in the new representation space, you don’t have to explicitly compute
the coordinates of your points in the new space; you just need to compute the distance between pairs of points in that space, which can be done efficiently using a kernel function.

A kernel function is a computationally tractable operation that maps any
two points in your initial space to the distance between these points in your target
representation space, completely bypassing the explicit computation of the new representation.

At the time they were developed, SVMs exhibited state-of-the-art performance on
simple classification problems and were one of the few machine-learning methods
backed by extensive theory and amenable to serious mathematical analysis, making
them well understood and easily interpretable.

But SVMs proved hard to scale to large datasets and didn’t provide good results for
perceptual problems such as image classification. Because an SVM is a shallow
method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle.

What is a decision tree, a random forest and gradient boosting machine ?

Decision trees are flowchart-like structures that let you classify input data points or predict output values given inputs.

the Random Forest algorithm introduced a robust, practical take on
decision-tree learning that involves building a large number of specialized decision
trees and then ensembling their outputs. Random forests are applicable to a wide
range of problems—you could say that they’re almost always the second-best algorithm
for any shallow machine-learning task.

A gradient boosting machine, much like a random forest, is a machine-learning
technique based on ensembling weak prediction models, generally decision trees. It uses gradient boosting, a way to improve any machine-learning model by iteratively training new models that specialize in addressing the weak points of the previous models.
Applied to decision trees, the use of the gradient boosting technique results in models
that strictly outperform random forests most of the time, while having similar properties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual
data today.



 
























